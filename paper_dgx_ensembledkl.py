# -*- coding: utf-8 -*-
"""Paper_DGX_EnsembleDKL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19zoTb16mY310UcQDPFMZO_8p39WVnFIV
"""

###################################
###################################
#                        **Active Channel Learning with Ensembles of Deep Kernel Learning Models**

#-----This workflow is designed by Yongtao Liu [https://youngtaoliu.wixsite.com/yongtaoliu; https://scholar.google.com/citations?user=V9FMPgQAAAAJ&hl=en] based on 
#---Ensemble DKL model from Maxim Ziatdinov [https://scholar.google.com/citations?hl=en&user=YnSdOoUAAAAJ&view_op=list_works&sortby=pubdate]

###################################
###################################


print('Import')
import time
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import gpax
import haiku as hk
import jax
import jax.numpy as jnp
from typing import Tuple
from warnings import filterwarnings
from sklearn.model_selection import train_test_split
from mlsocket import MLSocket

gpax.utils.enable_x64()

#########################################################################################

def user_manual():
  print('''\n#####################---User Manul---#####################
  Ensemble-DKL notebook (nb#1) running on GPU server includes 4 steps: s1, check_first_point, s2, and s3. 
  This noteobok needs to be used with a partner notebook (nb#2) running on local computer to control V3.
  1. After V3 finishes a BEPFM measurement, nb#2 needs be used to make $all_channels.npy$ file that includes 
  ampltide, phase, frequency, and topography channel images of BEPFM data.
  2. Then, upload $all_channels.npy$ to the same directory of nb#1 on GPU server.
  3. Run s1() in nb#1 to prepare training data, and seed-point coordinates named as $indices_train.npy$.
  4. Download $indices_train.npy$ to local computer, and use nb#1 and V3 to get measurement results of these seed points.
  5. V3 starts the measurement from a random location, so please load this random location with nb#1 function check_first_point().
  6. Seed-point results data is named as $y_train_pr.npy$, load this file to GPU server.
  7. Run nb#1 s2() to update training data set.
  8. Run s3() to start Ensemble-DKL driven measurement.
  Note: there are two help functions in nb#1 named check_variables() and check_dkl_parameters() allow user to look at varibles and experiment parameter''')

##############################################################################

#Help functions 
def make_window(imgsrc: np.ndarray, window_size: int,
                xpos: int, ypos: int) -> np.ndarray:
    """
    Returns the portion of the image within the window given the
    image (imgsrc), the x position and the y position
    """
    imgsrc = imgsrc[int(xpos-window_size/2):int(xpos+window_size/2),
                    int(ypos-window_size/2):int(ypos+window_size/2)]
    return imgsrc

def create_training_set(imgdata: np.ndarray, target: np.ndarray,
                        window_size: int) -> Tuple[np.ndarray]:
    """
    Creates arrays with features (local subimages)
    and targets (corresponding spectra) from hyperspectral data
    """
    feature_arr, target_arr = [], []
    pos = []
    s1, s2 = imgdata.shape[:-1]
    for i in range(s1):
        for j in range(s2):
            arr_loc = make_window(imgdata, window_size, i, j)
            if arr_loc.shape[:-1] != (window_size, window_size):
                continue
            feature_arr.append(arr_loc)
            target_arr.append(target[i, j])
            pos.append([i, j])
    return np.array(pos), np.array(feature_arr), np.array(target_arr)

def create_training_set2(imgdata: np.ndarray, window_size: int) -> Tuple[np.ndarray]:
    """
    Creates arrays with features (local subimages)
    and targets (corresponding spectra) from hyperspectral data
    """
    feature_arr = []
    pos = []
    s1, s2 = imgdata.shape[:-1]
    for i in range(s1):
        for j in range(s2):
            arr_loc = make_window(imgdata, window_size, i, j)
            if arr_loc.shape[:-1] != (window_size, window_size):
                continue
            feature_arr.append(arr_loc)
            pos.append([i, j])
    return np.array(pos), np.array(feature_arr)
class MLP2(hk.Module):
    """Simple custom MLP"""
    def __init__(self, embedim=2):
        super().__init__()
        self._embedim = embedim

    def __call__(self, x):
        x = hk.Linear(64)(x)
        x = jax.nn.tanh(x)
        x = hk.Linear(32)(x)
        x = jax.nn.tanh(x)
        x = hk.Linear(self._embedim)(x)
        return x

def step(X_train, y_train, X_unmeasured):
    """Single Ensemble-DKL step"""
    key, _ = gpax.utils.get_keys() 
    dkl = gpax.viDKL(X_train.shape[-1], z_dim=2, kernel='RBF', nn=MLP2)
    y_pred, covar_d = dkl.fit_predict(
        key, X_train, y_train, X_unmeasured,
        n_models=20, num_steps=2500, step_size=0.001)
    return y_pred.mean(0), covar_d.mean(0)

def get_best_channel(record):
    return record[:,1].argmax()

def update_record(record, action, r):
    new_r = (record[action, 0] * record[action, 1] + r) / (record[action, 0] + 1)
    record[action, 0] += 1
    record[action, 1] = new_r
    return record

def get_reward(obj_history, obj):
    """A reward of +/-1 is given if the integral uncertainty at the current step
    is smaller/larger than the integral uncertainty at the previous step"""
    if jnp.nansum(obj) < obj_history[-1]:
        r = 1
    else:
        r = -1
    return r

def update_datapoints2(next_point_idx, train, test, measured_result):
    X_train, y_train, indices_train = train
    X_test, indices_test = test
    X_train = jnp.append(X_train, X_test[:, next_point_idx:next_point_idx+1], axis=1)
    X_test = jnp.delete(X_test, next_point_idx, axis=1)
    measured = np.asarray([[measured_result, measured_result, measured_result, measured_result]]).T
    y_train = jnp.append(y_train, measured, axis=-1)
    #y_test = jnp.delete(y_test, next_point_idx, axis=-1)
    indices_train = jnp.append(indices_train, indices_test[next_point_idx:next_point_idx+1], axis=0)
    indices_test = jnp.delete(indices_test, next_point_idx, axis=0)
    return (X_train, y_train, indices_train), (X_test, indices_test)

###check if using GPU
def check_gpuORcpu():
  print(jax.devices())

######**********######
#prepare seed-points and training data
#global variables
all_channels = "not defined"
loop_areas = "not defined"
indices_all = "not defined"
features_all = "not defined"
targets_all = "not defined"
X_train = "not defined"
X_test = "not defined"
y_train = "not defined"
X_unmeasured = "not defined"
indices_unmeasured = "not defined"
indices_train = "not defined"
image_size = 128
patch_size = 20
random_seed = 1
m_start_time = 0

#####--Help functions to check parameters and variables--#####
def check_variables():
  global X_train, y_train, indices_train, indices_unmeasured, X_unmeasured
  print('X_train shape: ', X_train.shape)
  print('indices_unmeasured shape: ', indices_unmeasured.shape)
  print('y_train shape: ', y_train.shape)
  print('indices_train shape: ', indices_train.shape)
  print('y_train: ', y_train)
  print('X_unmeasured: ', X_unmeasured.shape)
  print('indices_unmeasured shape: ', indices_unmeasured)

def check_dkl_parameters():
  global warmup_steps, exploration_steps, eps
  print('##*##*##*##\n Ensemble DKL parameters')
  print('Warmup_steps: ', warmup_steps)
  print('Exploration_steps: ', exploration_steps)
  print('eps: ', eps)

#####---step 1: prepare image patch, training data, etc---#####
def s1 ():
  global m_start_time
  m_start_time = time.time()
  print("Measurement start")
  print("step 1: prepare image patch, training data set, seed points, etc")
  global all_channels, loop_area, indices_all, features_all, targets_all, image_size, patch_size 
  global indices_train, indices_unmeasured, X_train, y_train, X_unmeasured, random_seed
   
  
  all_channels = np.load('all_channels.npy')
  image_size = int(input("image size: "))
  patch_size = int(input("patch size: "))
  loop_area = np.zeros((image_size, image_size))
  indices_all, features_all, targets_all = create_training_set(all_channels, 
                                                               loop_area, patch_size)

  #extract patches
  n, h, w, c = features_all.shape
  features_all = features_all.reshape(n, h*w, c)

  #seed-points
  train_ratio = float(input("train data percentage: "))
  train_ratio = train_ratio/100
  random_seed = int(input("train data split random seed: "))
  (X_train, X_unmeasured, y_train, 
   y_unmeasured, indices_train, 
   indices_unmeasured) = train_test_split(features_all, 
                                          targets_all, indices_all, 
                                          test_size=1-train_ratio, random_state=random_seed)

  X_train = X_train.transpose(2, 0, 1)
  X_unmeasured = X_unmeasured.transpose(2, 0, 1)
  y_train = y_train[None].repeat(X_train.shape[0], axis=0)

  np.save("indices_train.npy", indices_train)

  print('X_train shape: ', X_train.shape)
  print('X_unmeasured shape: ', X_unmeasured.shape)
  print('y_train shape: ', y_train.shape)
  print('indices_train shape: ', indices_train.shape)
  print('#####\n\n\nPlease perform seed point measurements')
  print('Please remember to add first random point')

######**********######
#After finishing seed-points measurement, update y_train
y_train_pr = "not defined"
ran_indices = 'not defined'

  #V3 starts frist measruementa t random location, add this random location here
def check_first_point(x_idx, y_idx):
  print("check and add first random point from V3")
  global indices_unmeasured, ran_indices

  ran_indices = np.asarray([x_idx, y_idx])
  # ran_indices = np.load("/.../first_point.npy")
  i1 = np.where(indices_unmeasured[:,0]==ran_indices[0])
  i2 = np.where(indices_unmeasured[:,1]==ran_indices[1])
  next_point_idx = np.intersect1d(i1[0], i2[0])[0]
  print('first point and indices: ', x_idx, y_idx, next_point_idx)

def s2():
  print("step 2: load seed points results")
  global y_train_pr, y_train, indices_unmeasured, X_train, indices_train, X_unmeasured, ran_indices

  y_train_pr = np.load("y_train_pr.npy")
  y_train_ = (y_train_pr-y_train_pr.min())/y_train_pr.ptp()
  y_train = y_train_[1:][None].repeat(X_train.shape[0], axis = 0)
  
  #We need to add the first random point result to train dataset
  i1 = np.where(indices_unmeasured[:,0]==ran_indices[0])
  i2 = np.where(indices_unmeasured[:,1]==ran_indices[1])
  next_point_idxx = np.intersect1d(i1[0], i2[0])[0]
  print('first point indices: ', next_point_idxx)
  measured_point = y_train_[0]
  (X_train, y_train, indices_train), (X_unmeasured, indices_unmeasured) = update_datapoints2(
      next_point_idxx, (X_train, y_train, indices_train), 
      (X_unmeasured, indices_unmeasured), measured_point)

  print('##*##*##*##\n Seed point measurement finished')
  print('X_train shape: ', X_train.shape)
  print('y_train shape: ', y_train.shape)
  print('indices_train shape: ', indices_train.shape)
  print('y_train: ', y_train)
  print('X_unmeasured shape: ', X_unmeasured.shape)
  print('indices_unmeasured shape: ', indices_unmeasured.shape)

######**********######
#experiment
record = 'not defined'
channel_choices = 'not defined'
var_history = 'not defined'
warmup_steps = 'not defined'
exploration_steps = 'not defined'
eps = 'not defined'
savedir = "not defined"

def s3():
  print("step 3: set warmup and exploration steps, then bind DGX-GPU to local PC and start experiment")
  global warmup_steps, exploration_steps, record, var_history, m_start_time
  global measuredpoint, record, channel_choices, savedir
  global X_train, X_unmeasured, y_train, indices_train, indices_unmeasured

  np.random.seed(55) # rng seed for epsilon-greedy sampling
  record = np.zeros((X_train.shape[0], 2))
  channel_choices = []
  var_history = []
  
  #save directory
  print('Provide sample and loop names to set the data save directory')
  sample_name = input("sample name:")
  loop_name = input("loop name:")
  savedir = "/AE/{}_{}_record".format(sample_name, loop_name)
  #Ensemble DKL parameters
  warmup_steps = int(input("Warmup step: "))
  exploration_steps = int(input("Exploration step: "))
  eps = np.linspace(0.4, 0.1, exploration_steps)  #epsilon in epsilon-greedy policy

  HOST = ''
  PORT = 3446
  
  print('jax device: ', jax.devices())

  with MLSocket() as s:
    s.bind((HOST, PORT))
    s.listen()
    conn, address = s.accept()
    print("bind successfully")
    with conn:
      # Warm-up phase
      print('Warm-up starts')
      for w in range(warmup_steps):
        t0 = time.time()
        print("###########--Warmup step {}/{}--###########".format(w+1, warmup_steps))
        # Get/update DKL posteriors
        #y_train = (y_train-y_train.min())/y_train.ptp()
        print("y_trian:")
        print(y_train[:8], "......", y_train[-8:])
        print("training starts")
        mean, var = step(X_train, y_train, X_unmeasured)
        print("training finishes")
        record[:, 0] += 1
        # Select channel that resulted in lowest predictive uncertainty
        var_reduced = np.nansum(var, axis=-1)
        idx = np.argmin(var_reduced)
        # Update records
        var_history.append(var_reduced[idx].item())
        channel_choices.append(idx)
        record[idx, 1] += 1
        # get the next measurement point
        obj = var[idx]
        next_point_idx = obj.argmax()

        # np.save('next_point.npy', indices_unmeasured[next_point_idx,])
        nextpoint = np.asarray(indices_unmeasured[next_point_idx,])
        time.sleep(0.01)
        conn.send(nextpoint)
        time.sleep(0.01)
        print("send next point idx and next point: ", next_point_idx, nextpoint)
        #save step record
        #plot_acq(st=w)
        obj_np=np.asarray(obj)
        np.save(os.path.join(savedir, 'objmean{}.npy'.format(w)), obj_np)
        np.save(os.path.join(savedir, 'indices_unmeasured{}.npy'.format(w)), indices_unmeasured)
    
        # Do "measurement"
        measured_point = conn.recv(920)
        time.sleep(0.01)
        print("received new data")

        # update datapoints
        (X_train, y_train, indices_train), (X_unmeasured, 
                                            indices_unmeasured) = update_datapoints2(
                                                next_point_idx, (X_train, y_train, indices_train), 
                                                (X_unmeasured, indices_unmeasured), measured_point)

        print("total time in this step: ", time.time()-t0)

      #save record
      np.save(os.path.join(savedir, "ensembleDKL_warmuprecords{}.npy".format(w)), record)

      # Average over the number of warmup steps
      record[:, 1] = record[:, 1] / warmup_steps

      # Print the running rewards
      print("\nRewards:")
      for i, r in enumerate(record):
        print("channel {}:  counts {}  reward (avg) {}".format(i+1, (int(r[0])), np.round(r[1], 3)))

# Exploration-exploitation phase
      print('Warmup ends, exploration starts')
      for e in range(exploration_steps):
        t0 = time.time()
        print("\n###########--Exploration step {}/{}--###########".format(e+1, exploration_steps))
    # Choose channel according to epsilon-greedy policy
        eps_i = eps[e] if e < exploration_steps else eps[-1]
        if np.random.random() > eps_i:
          idx = get_best_channel(record)
        else:
          idx = np.random.randint(record.shape[0])
        channel_choices.append(idx)
        print("Using channel {}".format(idx+1))
        # Update DKL posterior using the sampled channel
        print("training starts")
        mean, var = step(X_train[idx], y_train[idx], X_unmeasured[idx])
        print("training finishes")
        # Get reward
        r = get_reward(var_history, var)
        # Update records
        record = update_record(record, idx, r)
        var_history.append(jnp.nansum(var).item())

        # Get the next measurement point
        # obj = mean + 0.5 * jnp.sqrt(var) - UCB acqusition function
        obj = var
        next_point_idx = obj.argmax()

        nextpoint = np.asarray(indices_unmeasured[next_point_idx,])
        time.sleep(0.1)
        conn.send(nextpoint)
        time.sleep(0.01)
        print("send next point idx and next point: ", next_point_idx, nextpoint)
      
        obj_np=np.asarray(obj)
        np.save(os.path.join(savedir, 'objmean{}.npy'.format(warmup_steps+e)), obj_np)
        np.save(os.path.join(savedir, 'indices_unmeasured{}'.format(warmup_steps+e)), indices_unmeasured)

        #do measurement
        measured_point = conn.recv(920)
        time.sleep(0.01)
        print("new data received")

        # update datapoints
        (X_train, y_train, indices_train), (X_unmeasured, indices_unmeasured) = update_datapoints2(
            next_point_idx, (X_train, y_train, indices_train), 
            (X_unmeasured, indices_unmeasured), measured_point)
        # Print the running rewards
        print("\nRewards:")
        for i, r in enumerate(record):
          print("channel {}:  counts {}  reward (avg) {}".format(i+1, 
                                                                 (int(r[0])), 
                                                                 np.round(r[1], 3)))

      #save record
        np.save(os.path.join(savedir, "ensembleDKL_records{}.npy".format(e)),record)
        print("total time in this step: ", time.time()-t0)
    
    conn.close()
    s.shutdown(1)
    s.close()
    
    # Store all points
    np.savez(os.path.join(savedir, "ensembleDKL_channels_norm.npz"),
             X_train=X_train, y_train=y_train, X_unmeasured=X_unmeasured,
             indices_unmeasured=indices_unmeasured, final_mean = mean, final_var = var,
             indices_measured=indices_train, record=record, channels_id=channel_choices,
             var_history=var_history)
    print("Measurement done\nPlease export saved data")
    total_t = time.time()-m_start_time
    th = int(total_t/3600)
    tm = int((total_t%3600)/60)
    ts = int(((total_t%3600)%60))
    print("Total time of this measurement: {}h {}m {}s: ".format(th, tm, ts))